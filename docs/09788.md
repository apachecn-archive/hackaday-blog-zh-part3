# 死亡 Vs 误报:特斯拉和优步撞车事故的教训

> 原文:[https://hack aday . com/2018/06/18/死亡人数与误报人数之比-特斯拉和优步撞车事故的教训/](https://hackaday.com/2018/06/18/fatalities-vs-false-positives-the-lessons-from-the-tesla-and-uber-crashes/)

在 3 月份糟糕的一周，有两人被自动驾驶系统间接撞死。一辆特斯拉汽车撞上了路障，导致司机死亡，一辆优步汽车撞上并撞死了一名过马路的行人。美国国家运输安全委员会关于这两起事故的初步报告最近出炉了，这些报告让我们尽可能接近于对实际发生的事情有一个明确的看法。从这两起坠机事件中我们能学到什么？

有一个突出的因素使这两起碰撞在表面上看起来不同:特斯拉的算法错误地识别了车道分裂，并主动加速进入了障碍，而优步系统最终正确地识别了骑自行车的人穿过街道，可能有时间停下来，但它被禁用了。你可能会说，如果特斯拉司机死于对系统的过度信任，那么优步的死亡则源于对系统的过度信任。

但是你错了。特斯拉*中的前向雷达本应*通过看到障碍物并紧急刹车来防止事故发生，但特斯拉算法对摄像头的重视超过了雷达。为什么？与优步紧急制动系统被关闭的原因完全一样:有“太多”的误报，结果是在正常驾驶情况下，汽车经常进行不必要的制动。

目前自动驾驶的关键是精确地弄清楚什么时候应该急刹车，什么时候不应该。刹车太频繁，乘客会不高兴或者汽车被追尾。刹车太不频繁，后果会更糟。事实上，这是自动驾驶汽车安全的核心问题，特斯拉和优步都还没有解决。

## 特斯拉汽车撞上停止的物体

先说[特斯拉撞车](https://www.ntsb.gov/investigations/AccidentReports/Pages/HWY18FH011-preliminary.aspx)。就在撞车之前，这辆车使用其交通感知巡航控制系统跟在另一辆车后面，试图保持给定的速度，与前面的车保持适当的跟随距离。当特斯拉接近出口匝道时，前面的汽车靠右行驶，特斯拉向左移动，被车道分割线上的车道标记弄糊涂了，并加速到 75 英里/小时(120 公里/小时)的编程速度，而没有注意到前面的障碍。简单来说，算法出错，全速驶入车道分割线。

[![](../Images/0169b3c1f51c6a43a31af019e493151f.png)](https://hackaday.com/wp-content/uploads/2018/06/mpv-shot0001_thumbnail.png) 平心而论，汽车的混乱是可以理解的。事件发生后，很自然地，许多硅谷特斯拉司机在自己的车上重现了这个“实验”，并在 YouTube 上发布了视频。在[这幅](https://www.youtube.com/watch?time_continue=65&v=VVJSjeHDvfY)中，你可以看到分道线的右边条纹明显比左边条纹更难看到。这就解释了为什么这辆车认为它在车道上，而实际上它是在“gore”里——出口匝道前的三角形禁区。(从同一视频中，你还可以看到任何人类驾驶员如何本能地跟随前面的汽车，而不是被一些缺失的油漆拉离轨道。)

更令人担忧的是，芝加哥一个类似的匝道[愚弄了一辆特斯拉，让它做出了完全相同的行为](https://www.youtube.com/watch?v=6QCF8tVqM3I)(又是 YouTube)。当你相信计算机视觉的时候，你就等于把你的生命押在了道路上所画条纹的质量上。

正如我在上面提到的，特斯拉事故中的棘手问题是，当它看到混凝土障碍时，为什么它的雷达没有及时超越和刹车。这一点可以在 2018 年 1 月的[特斯拉以 65 英里/小时(105 公里/小时)的速度追尾一辆停下来的消防车](https://www.wired.com/story/tesla-autopilot-why-crash-radar/)的案例中找到线索。)、一辆[特斯拉撞上一辆停着的警车](https://www.bbc.com/news/technology-44300952)，甚至还有 2016 年[第一起特斯拉致死案](https://www.ntsb.gov/investigations/AccidentReports/Pages/HWY16FH018-preliminary.aspx)，当时“自动驾驶”直接撞上了一辆半挂车的侧面。车主手册中的一句名言:“交通感知巡航控制系统无法检测所有物体，并且可能无法对静止车辆进行制动/减速，尤其是在车速超过 50 英里/小时(80 公里/小时)的情况下……”的确如此。

特斯拉的算法可能不信任雷达，因为雷达数据充满了误报。高速公路上有无数不动的物体:街道标志、停放的汽车和水泥车道分隔带。简单雷达的角分辨率很低，这意味着在高速行驶时，雷达也“看到”静止的物体，而汽车无论如何也不会撞到这些物体。正因为如此，为了防止汽车在每个街边广告牌上急刹车，特斯拉的系统在速度上更加重视视觉信息。因为特斯拉的“自动驾驶”并不打算成为自动驾驶的解决方案，他们可以躲在遮羞布后面，司机应该已经看到它的到来。

## 优步禁用紧急制动

[![](../Images/e5f9706d3747d5450dcf798f0725dc76.png)](https://hackaday.com/wp-content/uploads/2018/06/hwy18mh010-prelim-fig2.png) 在特斯拉事故中，人类司机本可以避免失明，与之相反，[优步汽车可能及时刹车，完全避免了事故的发生](https://www.ntsb.gov/investigations/AccidentReports/Pages/HWY18MH010-prelim.aspx)，而人类司机不可能做到这一点。激光雷达系统早在撞击前六秒钟就捕捉到了行人，将她不同地分类为“未知物体，车辆，然后是对未来行驶路径有不同预期的自行车。”即便如此，当汽车距离撞击还有 1.3 秒和 25 米时，优步系统非常确定地得出结论，需要紧急制动。不幸的是，他们没有订婚。

在 25 米(82 英尺)内从每小时 43 英里(69 公里/小时)的速度刹车 [*仅仅是*可行的，只要你在有好轮胎的干燥路面上猛踩刹车](http://www.csgnetwork.com/stopdistinfo.html)。然而，一旦你把人类的平均反应时间加入到这个等式中，一个人就不可能完成它。事实上，NTSB 的报告提到，司机在撞击前不到一秒钟突然转向，而她在撞击后不到一秒钟就踩了刹车。她可能被系统自己的日志和报告界面分散了注意力，她被要求用作测试驱动程序，但她的反应完全是人类的:只是有点太晚了。(如果她比激光雷达更早察觉到行人并预计到她正在走上街道，事故也可以避免。)

但优步紧急制动系统并没有启用，以“减少车辆不稳定行为的可能性”。也就是说，优步的激光雷达系统，像特斯拉的雷达一样，显然也存在误报问题。

## 死亡与假阳性

在任何统计系统中，就像自动驾驶汽车内部运行的分类算法一样，你可能会犯两种截然不同的错误:检测到自行车并在没有自行车时刹车，以及在有自行车时未能检测到自行车。想象一下，你正在调整其中一个算法，让它在街上行驶。如果你把声明一个物体是一辆自行车的门槛设得很低，你会犯很多第一类错误——误报——并且你会经常不必要的刹车。如果你提高“骑车”的门槛以减少假阳性的数量，你必然会增加错过一些实际自行车的风险，并产生更多的假阴性错误，可能会撞上更多的骑自行车者或水泥障碍。

[![](../Images/e85c449e16bce65766308fa954e6f6d1.png)](https://hackaday.com/wp-content/uploads/2018/06/tesla_inside.png)

Source: S. Engleman, in NTSB report

从纯统计的角度来看待这种生死攸关的决定似乎有些冷酷，但事实是，在假阳性和假阴性之间存在不可避免的设计权衡。自动驾驶汽车系统的设计者面临着这一艰难的选择——权衡日常驾驶体验与假阴性的难以置信的罕见但更可怕的结果。特斯拉在面临雷达的高误报率时，选择更多地依赖计算机视觉系统。优步的激光雷达系统显然产生了过于频繁的紧急制动操作，他关闭了系统，并将负载直接放在了司机身上。

当然，过高的误报率也会带来危险:如果你前面的一辆车莫名其妙地紧急刹车，你可能会撞到它。频繁刹车的影响不仅仅是给驾驶员带来不便，还可能成为事故的另一个原因。(的确，Waymo 和通用汽车的巡航自动驾驶汽车比平均水平更容易被人类司机撞上，但这是另一回事了。)随着自驾车在分类方面变得更好，这两种错误率都可以降低，这可能会使未来的权衡变得更容易，但总会有权衡，错误率永远不会为零。

没有这些数字，我们甚至无法判断特斯拉或优步的权衡方法是否恰当。特别是因为假阴性的后果可能是致命的，并且涉及到除司机之外的其他人，这种权衡影响到每个人*，可能应该更透明地讨论。如果一家公司在假阴性率上反复无常，司机和行人会不必要地死去，但如果他们太严格，汽车将无法驾驶，也不稳定。泰斯拉和优步在面对这一艰难的权衡时都放弃了:他们[需要一个人来提防假阴性](https://hackaday.com/2016/12/05/self-driving-cars-are-not-yet-safe/)，卸下机器的负担。*