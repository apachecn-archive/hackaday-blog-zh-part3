# 围绕神经网络思考

> 原文:[https://hack aday . com/2017/05/22/wrap-your-mind-around-neural-networks/](https://hackaday.com/2017/05/22/wrap-your-mind-around-neural-networks/)

人工智能在文明国家的生活中扮演着越来越重要的角色，尽管大多数公民可能没有意识到这一点。现在打电话给公司时用电脑说话已经很平常了。脸书在识别上传照片中的人脸方面变得惊人的准确。与智能手机的物理交互正在成为过去……有了苹果的 Siri 和谷歌语音，简单地与你的手机交谈并告诉它做什么比键入或触摸图标更容易，但肯定会变得更容易。如果你以前没有试过，试试这个——如果你有安卓手机，说“OK Google”，然后说“Lumos”。太神奇了！

我们感兴趣的产品广告会在我们的社交媒体账户上弹出[,好像有什么东西在读我们的思想](http://hackaday.com/2017/03/10/creepy-ai/)。事实是，有东西*在*读取我们的思想……尽管很难确切确定*到底是什么东西*。我们想要的东西可能会出现广告，即使我们在看到它之前从未意识到我们想要它。这不是巧合，而是源于一个 AI 算法。

许多人工智能应用的核心是一个被称为深度学习的过程。最近有很多关于深度学习的讨论，不仅是在 hack aday 这里，而是遍布互联网。就像大多数与人工智能相关的事情一样，如果没有强大的计算机科学背景，它可能会有点复杂和难以理解。

如果你熟悉我的量子论文章，你会知道我喜欢研究复杂的主题，尽我所能去除复杂性，并以任何人都能理解的方式解释它。本文的目标是将类似的方法应用于深度学习的想法。如果神经网络让你斗鸡眼，机器学习让你做噩梦，请继续阅读。你会看到“深度学习”听起来像一个令人生畏的主题，但实际上只是一个 20 美元的术语，用来描述基础相对简单的东西。

## 机器学习

当我们给机器编写执行任务的程序时，我们编写指令，机器执行指令。例如，LED 亮… LED 灭…机器在完成指令后不需要知道预期的结果。机器没有理由知道 LED 是亮还是灭。它只是做你让它做的事。有了机器学习，这个过程就颠倒了。我们告诉机器我们想要的结果，然后机器‘学习’指令来达到目的。有多种方法可以做到这一点，但让我们关注一个简单的例子:

![](../Images/cf4db50f911932f9007648c3f8e3fc21.png)

Early neural network from [MIT](https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/)

如果我要你做一个小机器人，它可以自己引导自己到达目标，一个简单的方法是把机器人和目标放在 XY 笛卡尔平面上，然后给机器人编程，让它在 X 轴上走这么多单位，然后在 Y 轴上走这么多单位。这种简单的方法让机器人简单地执行指令，而不知道目标在哪里。只有当您知道起点和目标的坐标时，它才起作用。如果其中任何一个发生变化，这种方法就行不通了。

机器学习允许我们处理不断变化的坐标。我们告诉我们的机器人找到目标，并让它自己计算或学习到达目标的指令。一种方法是让机器人找到到目标的距离，然后随机移动。重新计算距离，回到开始的地方并记录距离测量值。重复这个过程将在从固定坐标移动后给我们几个距离测量值。在进行了 X 次测量后，机器人将向距离目标最短的方向移动，并重复该序列。这将最终使其达到目标。简而言之，机器人只是简单地使用试错法来“学习”如何到达目标。看，这东西并不难！

这种“通过试错学习”的想法可以抽象地用我们都听说过的东西来表示——神经网络。

## 假人的神经网络

神经网络得名于你脑袋里的大量神经元。尽管整个网络异常复杂，但单个神经元的操作却很简单。这是一个有几个输入和一个输出的细胞，化学电信号提供 IO。输出的状态由有效输入的数量和这些输入的*强度*决定。如果有足够多的有效输入，将会超过一个阈值，输出将变为有效。一个神经元的每个输出都充当另一个神经元的输入，从而创建了网络。

[![](../Images/faeda56166cb89e9d6357f322ee2ae67.png)](https://hackaday.com/wp-content/uploads/2017/05/perceptron-themed.png)

Perceptron diagram via [How to Train a Neuarl Network in Python](https://prateekvjoshi.com/2016/01/12/how-to-train-a-neural-network-in-python-part-i/) by Prateek Joshi

在硅中再造一个神经元(以及神经网络)应该也很简单。求和的时候有几个输入。将输入相加，如果超过特定阈值，则输出 1。否则输出零。答对了。虽然这让我们*有点像*模仿神经元，但不幸的是这不是很有用。为了让我们小小的硅神经元值得存储在闪存中，我们需要让输入和输出变得不那么二进制……我们需要赋予它们力量，或者更通俗的称呼:*权重。*

在 20 世纪 40 年代末，一个名叫弗兰克·罗森布拉特的人发明了一种叫做感知机的东西。除了一些例外，感知器就像我们在上一段描述的小硅神经元一样。其中最重要的是输入具有权重。随着权重的引入和一些反馈，我们获得了一个最有趣的能力……T2 学习 T3 的能力。

![](../Images/df843b4b2caf34d86043a888c46466c3.png)

Source via [KDnuggets](http://www.kdnuggets.com/2017/04/awesome-deep-learning-most-cited-papers.html)

倒回到我们学习如何到达目标的小机器人。我们给机器人一个结果，并让它编写自己的指令，以学习如何通过在 XY 坐标系中随机移动和距离测量的试错过程来实现该结果。感知器的概念是这个过程的抽象。人工神经元的输出就是我们的成果。我们希望神经元给我们一组特定输入的预期结果。我们通过让神经元调整输入的权重来实现这一点，直到它达到我们想要的结果。

通过称为反向传播的过程来调整权重，这是一种反馈形式。所以你有一组输入，一组权重和一个结果。我们计算结果离我们想要的有多远，然后使用这个差异(称为误差)来调整权重，使用的数学概念是梯度下降。这个“体重调整”过程通常被称为训练，但只不过是一个试错过程，就像我们的小机器人一样。

## 深度学习

如今深度学习的定义似乎比物联网还多。但我能找到的最简单、最直接的方法是*一种神经网络，在输入和输出之间有一层或多层，用于解决复杂问题*。基本上，深度学习只是一个复杂的神经网络，用来做传统计算机很难做的事情。

[![](../Images/161ecbb89f3e53ba7d5c2d43bd2acc33.png)](https://hackaday.com/wp-content/uploads/2017/05/deep-learning-diagram-themed.jpeg)

Deep Learning diagram via [A Dummy’s Guide to Deep Learning](https://medium.com/the-bleeding-edge/a-dummy-s-guide-to-deep-learning-part-1-of-3-ea8ae8d93e2a) by Kun Chen

输入和输出之间的层被称为隐藏层，并大大增加了神经网络的复杂性。每一层都有特定的用途，并按层次排列。例如，如果我们有一个经过训练的深度学习神经网络来识别图像中的一只猫，第一层可能会寻找特定的线段和圆弧。层次中较高的其他层将查看第一层的输出，并尝试识别更复杂的形状，如圆形或三角形。甚至更高层会寻找物体，比如眼睛或胡须。关于层次分类技术的更详细的解释，请务必查看我关于[不变表示的文章。](http://hackaday.com/2014/10/27/ask-hackaday-sequences-of-sequences/)

一个层的实际输出并不确切，因为它是通过试错过程训练的。用相同的图像训练的两个相同的深度学习神经网络将从其隐藏层产生不同的输出。这带来了一些令人不安的问题，因为[麻省理工学院正在发现](https://www.technologyreview.com/s/604087/the-dark-secret-at-the-heart-of-ai/)。

现在当你听到有人谈论机器学习、神经网络和深度学习时，你应该至少对它是什么有一个模糊的概念，更重要的是，它是如何工作的。神经网络似乎是下一件大事，尽管它们已经存在了很长时间。查看[Steven Dufresne]关于[这些年发生了什么变化](http://hackaday.com/2017/04/24/neural-networks-youve-got-it-so-easy/)的文章，并进入他关于[使用 TensorFlow 尝试机器学习的教程](http://hackaday.com/2017/04/11/introduction-to-tensorflow/)。