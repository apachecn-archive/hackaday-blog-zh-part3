# 自动驾驶汽车做出致命决定的伦理

> 原文:[https://hack aday . com/2015/10/29/the-ethics-of-self-driving-cars-making-deadly-decisions/](https://hackaday.com/2015/10/29/the-ethics-of-self-driving-cars-making-deadly-decisions/)

随着公司慢慢开始为商业市场测试和改进自动驾驶汽车，自动驾驶汽车开始在各地出现。见鬼，谷歌的无人驾驶汽车实际上在内华达州有自己的驾驶执照！事故很少，大多数时候，他们说这不是自动驾驶汽车的错。但是当自动驾驶汽车普及的时候——仍然会有事故——这是不可避免的。当你的车[不得不决定是救你还是一群人](http://arxiv.org/abs/1510.03346)时会发生什么？以前想过吗？

这是一个非常合理的担忧，并引发了一个巨大的伦理问题。在汽车不得不选择“最佳”结果的罕见情况下，什么将决定这一点？减少生命损失？即使这意味着撞到墙上，对你这个司机造成致命伤害？也许汽车制造商最终将不得不使弹射座椅成为标准功能！

如果这是所有商用自动驾驶汽车的标准配置，消费者真的会购买一辆为了拯救行人而以 0.00001%的概率被编程为杀死你的汽车吗？如果开车的是你，你能做出这样的选择吗？

嗯——研究人员决定对公众进行民意调查。这就是悖论所在——人们确实认为自动驾驶汽车应该为了多数人的需求而牺牲少数人的需求……只要他们自己不需要驾驶。这种困境实际上有一个名称。这叫做[电车问题。](https://en.wikipedia.org/wiki/Trolley_problem)

> 在场景 1 中，想象你正驾驶着一辆手推车沿着一组铁轨向一组五个工人走去。你的不作为会导致他们的死亡。如果你转向旁边的轨道，你会撞死一个正好挡在路上的工人。
> 
> 在场景 2 中，你是一名外科医生。五个人需要立即进行器官移植。你唯一的选择是从一个完全健康的第六个病人身上获取致命的所需器官，他不同意这个想法。如果你什么都不做，那五个病人都会死。

这两种情况的结果是一样的。杀一个救五个。但是这两者之间有道德上的区别吗？在一种情况下，你是一个英雄——你做了正确的事情。另一方面，你是个精神病患者。心理学不好玩吗？

正如研究论文的作者所说，他们的调查:

> 表明受访者可能会为自动驾驶汽车做好准备，这些汽车被编程为在不可避免的伤害情况下做出功利主义的道德决定……当谈到瞬间道德判断时，人们很可能对机器的期望比对彼此的期望更高。

Issac Asimov 的粉丝可能会觉得这很难理解:当被问及这些决定是否应该被载入法律时，大多数被调查者认为不应该。尽管创造法律来正式化这些道德决定的想法在自动驾驶汽车中比在人类驾驶的汽车中得到更多的支持，但它仍然遭到强烈反对。

然而，这确实创造了一个有趣的灰色区域:如果这些规则没有被法律强制执行，我们信任谁来创建做出这些决定的系统呢？你同意让谷歌做出这些生死抉择吗？

这真正意味着，在自动驾驶汽车商业化之前，公众舆论将不得不就自动驾驶汽车真正“可以”做什么(或不可以做什么)做出重大决定。这是一个非常有趣的问题，如果你有兴趣阅读更多内容，请查看另一篇关于如何帮助无人驾驶汽车做出道德决定的文章。

你可能会重新考虑[建造自己的无人驾驶汽车](http://hackaday.com/2012/02/05/build-your-own-self-driving-car/)吗？

[研究论文通过[麻省理工科技评论](http://www.technologyreview.com/view/542626/why-self-driving-cars-must-be-programmed-to-kill/)